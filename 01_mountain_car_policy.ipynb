{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the agent is learning on two continuous variables: position and velocity. For any given state\n",
    "(position and velocity) of the car, the agent is given the possibility of driving left, driving right,\n",
    "or not using the engine at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2.0e-1                 # Learning rate\n",
    "numberStates = 40              # Number of states\n",
    "maxEpisodes = 5000             # Episodes for which I am running the agent\n",
    "initialLearningRate = 1.0      # Initial learning rate\n",
    "maxStep = 10000                # Max step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.05\n",
    "gamma = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stateObservation(env, observation):\n",
    "    # Map an observation to state\n",
    "    envLow = env.observation_space.low\n",
    "    envHigh = env.observation_space.high\n",
    "    env_dx = (envHigh - envLow) / numberStates\n",
    "\n",
    "    # Observation[0]:position ;  Observation[1]: velocity\n",
    "    position = int((observation[0] - envLow[0]) / env_dx[0])\n",
    "    velocity = int((observation[1] - envLow[1]) / env_dx[1])\n",
    "    return position, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episodeSimulator(env, policy=None, render=False):\n",
    "    observation = env.reset()\n",
    "    totalReward = 0\n",
    "    stepCount = 0\n",
    "\n",
    "    for x in range(maxStep):\n",
    "        if policy is None:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            position, velocity = stateObservation(env, observation)\n",
    "            action = policy[position][velocity]\n",
    "        if render:\n",
    "            env.render()\n",
    "        # Proceed environment for each step\n",
    "        # Get observation, reward and done after each step\n",
    "        observation, reward, done, x = env.step(action)\n",
    "        totalReward += gamma ** stepCount * reward\n",
    "        stepCount += 1\n",
    "        if done:\n",
    "            break\n",
    "    return totalReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      " Iteration:   1  Reward:-210.0\n",
      " Iteration:  51  Reward:-212.0\n",
      " Iteration: 101  Reward:-208.0\n",
      " Iteration: 151  Reward:-207.0\n",
      " Iteration: 201  Reward:-206.0\n",
      " Iteration: 251  Reward:-213.0\n",
      " Iteration: 301  Reward:-1.0\n",
      " Iteration: 351  Reward:-210.0\n",
      " Iteration: 401  Reward:-213.0\n",
      " Iteration: 451  Reward:-210.0\n",
      " Iteration: 501  Reward:-211.0\n",
      " Iteration: 551  Reward:-213.0\n",
      " Iteration: 601  Reward:-208.0\n",
      " Iteration: 651  Reward:-209.0\n",
      " Iteration: 701  Reward:-215.0\n",
      " Iteration: 751  Reward:-211.0\n",
      " Iteration: 801  Reward:-210.0\n",
      " Iteration: 851  Reward:-205.0\n",
      " Iteration: 901  Reward:-1.0\n",
      " Iteration: 951  Reward:-207.0\n",
      " Iteration:1001  Reward:-218.0\n",
      " Iteration:1051  Reward:-212.0\n",
      " Iteration:1101  Reward:-210.0\n",
      " Iteration:1151  Reward:-218.0\n",
      " Iteration:1201  Reward:-215.0\n",
      " Iteration:1251  Reward:-219.0\n",
      " Iteration:1301  Reward:-212.0\n",
      " Iteration:1351  Reward:-212.0\n",
      " Iteration:1401  Reward:-208.0\n",
      " Iteration:1451  Reward:-206.0\n",
      " Iteration:1501  Reward:-208.0\n",
      " Iteration:1551  Reward:-212.0\n",
      " Iteration:1601  Reward:-211.0\n",
      " Iteration:1651  Reward:-210.0\n",
      " Iteration:1701  Reward:-208.0\n",
      " Iteration:1751  Reward:-212.0\n",
      " Iteration:1801  Reward:-213.0\n",
      " Iteration:1851  Reward:-213.0\n",
      " Iteration:1901  Reward:-217.0\n",
      " Iteration:1951  Reward:-208.0\n",
      " Iteration:2001  Reward:-211.0\n",
      " Iteration:2051  Reward:-205.0\n",
      " Iteration:2101  Reward:-215.0\n",
      " Iteration:2151  Reward:-215.0\n",
      " Iteration:2201  Reward:-209.0\n",
      " Iteration:2251  Reward:-211.0\n",
      " Iteration:2301  Reward:-213.0\n",
      " Iteration:2351  Reward:-211.0\n",
      " Iteration:2401  Reward:-210.0\n",
      " Iteration:2451  Reward:-210.0\n",
      " Iteration:2501  Reward:-208.0\n",
      " Iteration:2551  Reward:-207.0\n",
      " Iteration:2601  Reward:-212.0\n",
      " Iteration:2651  Reward:-207.0\n",
      " Iteration:2701  Reward:-208.0\n",
      " Iteration:2751  Reward:-211.0\n",
      " Iteration:2801  Reward:-205.0\n",
      " Iteration:2851  Reward:-210.0\n",
      " Iteration:2901  Reward:-214.0\n",
      " Iteration:2951  Reward:-211.0\n",
      " Iteration:3001  Reward:-205.0\n",
      " Iteration:3051  Reward:-204.0\n",
      " Iteration:3101  Reward:-212.0\n",
      " Iteration:3151  Reward:-213.0\n",
      " Iteration:3201  Reward:-206.0\n",
      " Iteration:3251  Reward:-1.0\n",
      " Iteration:3301  Reward:-210.0\n",
      " Iteration:3351  Reward:-204.0\n",
      " Iteration:3401  Reward:-212.0\n",
      " Iteration:3451  Reward:-207.0\n",
      " Iteration:3501  Reward:-210.0\n",
      " Iteration:3551  Reward:-211.0\n",
      " Iteration:3601  Reward:-173.0\n",
      " Iteration:3651  Reward:-172.0\n",
      " Iteration:3701  Reward:-214.0\n",
      " Iteration:3751  Reward:-215.0\n",
      " Iteration:3801  Reward:-207.0\n",
      " Iteration:3851  Reward:-212.0\n",
      " Iteration:3901  Reward:-210.0\n",
      " Iteration:3951  Reward:-210.0\n",
      " Iteration:4001  Reward:-208.0\n",
      " Iteration:4051  Reward:-212.0\n",
      " Iteration:4101  Reward:-215.0\n",
      " Iteration:4151  Reward:-210.0\n",
      " Iteration:4201  Reward:-210.0\n",
      " Iteration:4251  Reward:-210.0\n",
      " Iteration:4301  Reward:-179.0\n",
      " Iteration:4351  Reward:-214.0\n",
      " Iteration:4401  Reward:-209.0\n",
      " Iteration:4451  Reward:-212.0\n",
      " Iteration:4501  Reward:-210.0\n",
      " Iteration:4551  Reward:-212.0\n",
      " Iteration:4601  Reward:-211.0\n",
      " Iteration:4651  Reward:-214.0\n",
      " Iteration:4701  Reward:-206.0\n",
      " Iteration:4751  Reward:-1.0\n",
      " Iteration:4801  Reward:-211.0\n",
      " Iteration:4851  Reward:-211.0\n",
      " Iteration:4901  Reward:-209.0\n",
      " Iteration:4951  Reward:-213.0\n",
      "Mean-score :  -144.02\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "    reward = 0\n",
    "    done =0\n",
    "    # Create qTable with zeros\n",
    "    # 3 actions: 0:push_left, 1:no_push, 2:push_right\n",
    "    q_table = np.zeros((numberStates, numberStates, 3))\n",
    "\n",
    "    # Training for maximum iteration episodes\n",
    "    for i in range(maxEpisodes):\n",
    "        observation = env.reset()\n",
    "        totalReward = 0\n",
    "        # Learning rate is decreased at each step\n",
    "        eta = max(alpha, initialLearningRate * (0.85 ** (i // 100)))\n",
    "        # Each episode is max_step long\n",
    "        for j in range(maxStep):\n",
    "            position, velocity = stateObservation(env, observation)\n",
    "            # Select an action\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                # Get random action\n",
    "                action = np.random.choice(env.action_space.n)\n",
    "            else:\n",
    "                logits = q_table[position][velocity]\n",
    "                # Calculate the exponential of all elements in the input array.\n",
    "                logits_exp = np.exp(logits)\n",
    "                # Calculate the probabilities\n",
    "                probabilities = logits_exp / np.sum(logits_exp)\n",
    "                # Get random action\n",
    "                action = np.random.choice(env.action_space.n, p=probabilities)\n",
    "                # Get observation, reward and done after each step\n",
    "                observation, reward, done, _ = env.step(action)\n",
    "\n",
    "            totalReward += reward\n",
    "            # Update q table\n",
    "            # p:position, v:velocity\n",
    "            p_, v_ = stateObservation(env, observation)\n",
    "            # gamma: discount factor\n",
    "            # Bellmann equation: Q(s,a)=reward + gamma* max(Q(s_,a_))\n",
    "            q_table[position][velocity][action] = q_table[position][velocity][action] + eta * (\n",
    "                        reward + gamma * np.max(q_table[p_][v_]) - q_table[position][velocity][action])\n",
    "            if done:\n",
    "                break\n",
    "        if i % 50 == 0:\n",
    "            print(f\" Iteration:{i+1:4d}  Reward:{totalReward}\")\n",
    "\n",
    "    solutionPolicy = np.argmax(q_table, axis=2)\n",
    "\n",
    "    solutionPolicyScores = [episodeSimulator(env, solutionPolicy, False) for _ in range(100)]\n",
    "    print(\"Mean-score : \", np.mean(solutionPolicyScores))\n",
    "    # run with render=True for visualization\n",
    "    episodeSimulator(env, solutionPolicy, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the Simulation the car finally reaches the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
