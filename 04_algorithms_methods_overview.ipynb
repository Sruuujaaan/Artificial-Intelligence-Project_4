{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFS\n",
    "The Depth-first search (DFS) algorithm is a recursive algorithm that uses the idea of backtracking. Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. In the pacman project I have implemented DFS using a Stack. \n",
    "Pick a starting node and push all its adjacent nodes into a stack.\n",
    "Pop a node from stack to select the next node to visit and push all its adjacent nodes into a stack.\n",
    "Repeat this process until the stack is empty. However, ensure that the nodes that are visited are marked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BFS\n",
    "Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a 'search key’), and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.\n",
    "It uses the opposite strategy as depth-first search, which instead explores the highest-depth nodes first before being forced to backtrack and expand shallower nodes.  In the pacman project I have implemented BFS using a Queue. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCS\n",
    "Uniform Cost Search searches in branches which are more or less the same in cost. Uniform Cost Search demands the use of a priority queue. Unlike Depth First Search where the maximum depth had the maximum priority, Uniform Cost Search gives the minimum cumulative cost the maximum priority.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dijkstra’s \n",
    "Dijkstra’s algorithm can be regarded as a variant of uniform-cost search. In Dijkstra’s algorithm, there is no goal state and processing continue until all nodes have been removed from the priority queue. Dijkstra’s algorithm stops when the shortest paths to all nodes not just the goal state have been determined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Search\n",
    "A greedy algorithm is an algorithmic paradigm that follows the problem-solving heuristic of making the locally optimal choice at each stage with the intent of finding a global optimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aStarSearch\n",
    "A* algorithm is widely used in pathfinding and graph traversal, which is the process of finding a path between multiple points, called \"nodes\". The A* search algorithm is an extension of Dijkstra's algorithm useful for finding the lowest cost path between two nodes of a graph. The path may traverse any number of nodes connected by edges with each edge having an associated cost. The algorithm uses a heuristic which associates an estimate of the lowest cost path from this node to the goal node, such that this estimate is never greater than the actual cost. The algorithm should not assume that all edge costs are the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniMax\n",
    "Minimax is a kind of backtracking algorithm that is used in decision making and game theory to find the optimal move for a player, assuming that your opponent also plays optimally. In Minimax the two players are called maximizer and minimizer. The maximizer tries to get the highest score possible while the minimizer tries to do the opposite and get the lowest score possible. Every board state has a value associated with it. In each state if the maximizer has upper hand then, the score of the board will tend to be some positive value. If the minimizer has the upper hand in that board state, then it will tend to be some negative value. The values of the board are calculated by some heuristics which are unique for every type of game. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha-Beta Pruning\n",
    "Alpha-Beta pruning is not actually a new algorithm, rather an optimization technique for minimax algorithm. It reduces the computation time by a huge factor. This allows us to search much faster and even go into deeper levels in the game tree. It cuts off branches in the game tree which need not be searched because there already exists a better move available.\n",
    "Alpha is the best value that the maximizer currently can guarantee at that level or above.\n",
    "Beta is the best value that the minimizer currently can guarantee at that level or above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectimax search\n",
    "The expectimax algorithm is another variation of the minimax algorithm. Instead of taking the max or min of the utility values of their children, chance nodes take a weighted average, with the weight being the probability that child is reached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov decision process (MDP)\n",
    "A Markov decision process (MDP) is a discrete time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are used for studying optimization problems solved using dynamic programming and reinforcement learning.\n",
    "A Markov Decision Process (MDP) model contains:\n",
    "Set of possible world states S.\n",
    "Set of Models.\n",
    "Set of possible actions A.\n",
    "Real valued reward function R(s,a).\n",
    "Policy the solution of Markov Decision Process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. It does not require a model of the environment and can handle problems with stochastic transitions and rewards, without requiring adaptations. For any finite Markov decision process (FMDP), Q-learning finds a policy that is optimal in the sense that it maximizes the expected value of the total reward over all successive steps, starting from the current state. Q-learning can identify an optimal action-selection policy for any given FMDP, given infinite exploration time and a partly-random policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
